{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # A Deep Learning Approach to Predicting Cryptocurrency Prices (Ethereum, Bitcoin, etc.)\n",
    " \n",
    "##### We will Implement a recurrent neural network to predict bitcoin prices\n",
    "\n",
    "![alt text](https://dashee87.github.io/images/bitcoin_ether_training_test.png \"Logo Title Text 1\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Keras for deep learning\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Bidirectional\n",
    "from keras.models import Sequential\n",
    "\n",
    "## Scikit learn for mapping metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#for logging\n",
    "\n",
    "import time\n",
    "\n",
    "##matrix math\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "##plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##data processing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text]\n",
    "(http://2.bp.blogspot.com/-wuinSTn-X4A/UwHmmceDQqI/AAAAAAAAJFo/5EjPg-LpAJc/s1600/Sivakumar_Vellingiri_Normal_Forms_Poster.Jpeg \"Logo Title Text 1\")\n",
    "\n",
    "# Step 1 - Data Processing\n",
    "\n",
    "- The data is inputed as a .csv file.\n",
    "- Lets time-series transform data from (num days x num features) to (num days- window size x num days per sample x num features) where window size is 50\n",
    "- Normalization via dividing each value in the window by the first value of the window and then subtracting one.i.e [4,3,2] into [0, -0.25, -0.5]. .\n",
    "- The unnormalized bases are kept to compare the model's predictions of prices with the true prices. \n",
    "- The first 90% of the data is used in training the model, and the last 10% will be used to test the model. \n",
    "- A list of the prices before each day Y_test is drawn from will be compiled in order to generate statistics about the model's predictions\n",
    "\n",
    "======================================================================================================================\n",
    "\n",
    "The columns of data and their definitions are as follows: \n",
    "- Annual Hash Growth: Growth in the total network computations over the past 365 days\n",
    "- Block Height: The total number of blocks in the blockchain\n",
    "- Block Interval: Average amount of time between blocks\n",
    "- Block Size: The storage size of each block (i.e. megabytes)\n",
    "- BlockChain Size: The storage size of the blockchain (i.e. gigabytes)\n",
    "- Daily Blocks: Number of blocks found each day\n",
    "- Chain Value Density: The value of bitcoin's blockchain, in terms of dollars per megabyte\n",
    "- Daily Transactions: The number of transactions included in the blockchain per day\n",
    "- Difficulty: The minimum proof-of-work threshold required for a bitcoin miner to mine a block\n",
    "- Fee Percentage: Average fee paid as a percentage of transaction volume\n",
    "- Fee Rate: Average fee paid per transaction\n",
    "- Two-Week Hash Growth: Growth in the total network computations over the past 14 days\n",
    "- Hash Rate: The number of block solutions computed per second by all miners\n",
    "- Market Capitalization: The market value of all bitcoin in circulation\n",
    "- Metcalfe's Law - TX: A variant of Metcalfe's Law in which price is divided by n log n number of daily transactions\n",
    "- Metcalfe's Law - UTXO: A variant of Metcalfe's Law in which price is divided by n log n number of unspent transaction outputs\n",
    "- Miner Revenue Value: The amount of dollars earned by the mining network\n",
    "- Miner Revenue: The amount of bitcoin earned by the mining network, in the form of block rewards and transaction fees\n",
    "- Money Supply: The amount of bitcoin in circulation\n",
    "- Output Value: The dollar value of all outputs sent over the network\n",
    "- Output Volume: The amount of Bitcoin sent over the network\n",
    "- Bitcoin Price: The amount of dollars a single bitcoin is worth\n",
    "- Quarterly Hash Growth: Growth in the total network computations in the past 90 days\n",
    "- Total Transactions: The running total number of transactions processed by the Bitcoin network\n",
    "- Transaction Amount: The average amount of bitcoin moved per transaction\n",
    "- Fees Value: The dollar value of mining fees\n",
    "- Transaction Fees: The amount of bitcoin paid to miners in fees\n",
    "- Transaction Size: The average data size of a transaction\n",
    "- Transaction Value: The average dollar value moved in each transaction\n",
    "- Transactions per Block: The number of transactions in each block\n",
    "- Average UTXO Amount: The average amount of bitcoin contained in each unspent transaction output\n",
    "- UTXO Growth: The net number of unspent transaction outputs created\n",
    "- UTXO Set Size: The total number of unspent transaction outputs\n",
    "- Average UTXO Value: The average dollar value of each uspent transaction output\n",
    "- Velocity - Daily: The proportion of the money supply transacted each day\n",
    "- Velocity - Quarterly: The proportion of the money supply transacted each day, computed on a rolling-quarter basis\n",
    "- Velocity of Money: How many times the money supply changes hands in a given year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename, sequence_length):\n",
    "    \"\"\"\n",
    "    Loads the bitcoin data\n",
    "    \n",
    "    Arguments:\n",
    "    filename -- A string that represents where the .csv file can be located\n",
    "    sequence_length -- An integer of how many days should be looked at in a row\n",
    "    \n",
    "    Returns:\n",
    "    X_train -- A tensor of shape (2400, 49, 35) that will be inputed into the model to train it\n",
    "    Y_train -- A tensor of shape (2400,) that will be inputed into the model to train it\n",
    "    X_test -- A tensor of shape (267, 49, 35) that will be used to test the model's proficiency\n",
    "    Y_test -- A tensor of shape (267,) that will be used to check the model's predictions\n",
    "    Y_daybefore -- A tensor of shape (267,) that represents the price of bitcoin the day before each Y_test value\n",
    "    unnormalized_bases -- A tensor of shape (267,) that will be used to get the true prices from the normalized ones\n",
    "    window_size -- An integer that represents how many days of X values the model can look at at once\n",
    "    \"\"\"\n",
    "    #Read the data file\n",
    "    raw_data = pd.read_csv(filename, dtype = float).values\n",
    "    raw_data = raw_data[int(raw_data.shape[0]*0.5):]\n",
    "    \n",
    "    #Change all zeros to the number before the zero occurs\n",
    "    \n",
    "    for x in range(0, raw_data.shape[0]):\n",
    "        for y in range(0, raw_data.shape[1]):\n",
    "            if(raw_data[x][y] == 0):\n",
    "                raw_data[x][y] = raw_data[x-1][y]\n",
    "    \n",
    "    #Convert the file to a list\n",
    "    data = raw_data.tolist()\n",
    "    \n",
    "    #Convert the data to a 3D array (a x b x c) \n",
    "    #Where a is the number of days, b is the window size, and c is the number of features in the data file\n",
    "    result = []\n",
    "    for index in range(len(data) - sequence_length):\n",
    "        result.append(data[index: index + sequence_length])\n",
    "    \n",
    "    #Normalizing data by going through each window\n",
    "    #Every value in the window is divided by the first value in the window, and then 1 is subtracted\n",
    "    d0 = np.array(result)\n",
    "    dr = np.zeros_like(d0)\n",
    "    dr[:,1:,:] = d0[:,1:,:] / d0[:,0:1,:] - 1\n",
    "    \n",
    "    #Keeping the unnormalized prices for Y_test\n",
    "    #Useful when graphing bitcoin price over time later\n",
    "    start = 2400\n",
    "    end = int(dr.shape[0] + 1)\n",
    "    unnormalized_bases = d0[start:end,0:1,7]\n",
    "    \n",
    "    #Splitting data set into training (First 90% of data points) and testing data (last 10% of data points)\n",
    "    split_line = round(0.9 * dr.shape[0])\n",
    "    training_data = dr[:int(split_line), :]\n",
    "    \n",
    "    #Shuffle the data\n",
    "    np.random.shuffle(training_data)\n",
    "    \n",
    "    #Training Data\n",
    "    X_train = training_data[:, :-1]\n",
    "    Y_train = training_data[:, -1]\n",
    "    Y_train = Y_train[:, 7]\n",
    "    \n",
    "    #Testing data\n",
    "    X_test = dr[int(split_line):, :-1]\n",
    "    Y_test = dr[int(split_line):, 49, :]\n",
    "    Y_test = Y_test[:, 7]\n",
    "\n",
    "    #Get the day before Y_test's price\n",
    "    Y_daybefore = dr[int(split_line):, 48, :]\n",
    "    Y_daybefore = Y_daybefore[:, 7]\n",
    "    \n",
    "    #Get window size and sequence length\n",
    "    sequence_length = sequence_length\n",
    "    window_size = sequence_length - 1 #because the last value is reserved as the y value\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test, Y_daybefore, unnormalized_bases, window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.50715848e+09,   4.78096000e+05,   4.78359000e+05, ...,\n",
       "          9.33681341e+00,   4.46327572e+06,   4.78029872e+05],\n",
       "       [  1.50715854e+09,   4.77925000e+05,   4.77992000e+05, ...,\n",
       "          5.14419166e+00,   2.45846591e+06,   4.77911026e+05],\n",
       "       [  1.50715860e+09,   4.78004000e+05,   4.78220000e+05, ...,\n",
       "          5.04961210e-01,   2.41373186e+05,   4.78003422e+05],\n",
       "       ..., \n",
       "       [  1.51536948e+09,   1.94172500e+06,   1.94320000e+06, ...,\n",
       "          2.21347837e+00,   4.29994341e+06,   1.94261822e+06],\n",
       "       [  1.51536954e+09,   1.94320000e+06,   1.94497500e+06, ...,\n",
       "          1.43567333e+01,   2.79003865e+07,   1.94336594e+06],\n",
       "       [  1.51536960e+09,   1.94505000e+06,   1.94520000e+06, ...,\n",
       "          9.49936995e+00,   1.84703803e+07,   1.94437951e+06]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    raw_data = pd.read_csv(r'C:\\Users\\angel\\Desktop\\Predict Model\\Bitcoin Data.csv', dtype = float).values\n",
    "    raw_data[int(raw_data.shape[0]*0.5):]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Building the Model\n",
    "\n",
    "- We'll use a 3layer RNN with  20% dropout at each layer to reduce overfitting to the training data. \n",
    "- This model will have 515,579 trainable parameters throughout all of its layers. \n",
    "- The model uses the AdamOptimizer as its optimization function.\n",
    "- The loss function used in this model is mean squared error. \n",
    "- A linear activation function is used in this model to determine the output of each neuron in the model. The linear activation function is simply defined as f(x) = x.\n",
    "- The model will use Keras's Sequential model with Bidirectional LSTM layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/rnn.jpg \"Logo Title Text 1\")\n",
    "![alt text](https://docs.microsoft.com/en-us/azure/machine-learning/preview/media/scenario-tdsp-biomedical-recognition/lstm-cell.png \"Logo Title Text 1\")\n",
    "![alt text](http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/09/bidirectional-rnn.png \"Logo Title Text 1\")\n",
    "![alt text](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "Bidirectional RNNs are based on the idea that the output at time t may not only depend on the previous elements in the sequence, but also future elements. For example, to predict a missing word in a sequence you want to look at both the left and the right context. Bidirectional RNNs are quite simple. They are just two RNNs stacked on top of each other. The output is then computed based on the hidden state of both RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_model(window_size, dropout_value, activation_function, loss_function, optimizer):\n",
    "    \"\"\"\n",
    "    Initializes and creates the model to be used\n",
    "    \n",
    "    Arguments:\n",
    "    window_size -- An integer that represents how many days of X_values the model can look at at once\n",
    "    dropout_value -- A decimal representing how much dropout should be incorporated at each level, in this case 0.2\n",
    "    activation_function -- A string to define the activation_function, in this case it is linear\n",
    "    loss_function -- A string to define the loss function to be used, in the case it is mean squared error\n",
    "    optimizer -- A string to define the optimizer to be used, in the case it is adam\n",
    "    \n",
    "    Returns:\n",
    "    model -- A 3 layer RNN with 100*dropout_value dropout in each layer that uses activation_function as its activation\n",
    "             function, loss_function as its loss function, and optimizer as its optimizer\n",
    "    \"\"\"\n",
    "    #Create a Sequential model using Keras\n",
    "    model = Sequential()\n",
    "\n",
    "    #First recurrent layer with dropout\n",
    "    model.add(Bidirectional(LSTM(window_size, return_sequences=True), input_shape=(window_size, X_train.shape[-1]),))\n",
    "    model.add(Dropout(dropout_value))\n",
    "\n",
    "    #Second recurrent layer with dropout\n",
    "    model.add(Bidirectional(LSTM((window_size*2), return_sequences=True)))\n",
    "    model.add(Dropout(dropout_value))\n",
    "\n",
    "    #Third recurrent layer\n",
    "    model.add(Bidirectional(LSTM(window_size, return_sequences=False)))\n",
    "\n",
    "    #Output layer (returns the predicted value)\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    #Set activation function\n",
    "    model.add(Activation(activation_function))\n",
    "\n",
    "    #Set loss function and optimizer\n",
    "    model.compile(loss=loss_function, optimizer=optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Training the Model\n",
    "\n",
    "- The model will be fitted to the training dat with a batch_size of 1024. \n",
    "- Additionally, 100 epochs will be performed to give the model time to adjust its weights and biases to fit the training data.\n",
    "- 5% of the training data will be used as the validation set.  \n",
    "- The model will train by minimizing the loss (mean squared error) of its training data. -- - The validation set is useful when attempting to identify signs of overfitting. \n",
    "- If the validation loss begins to consistently and rapidly increase, the model has overfitted to the training data, and changes should be made to the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, Y_train, batch_num, num_epoch, val_split):\n",
    "    \"\"\"\n",
    "    Fits the model to the training data\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The previously initalized 3 layer Recurrent Neural Network\n",
    "    X_train -- A tensor of shape (2400, 49, 35) that represents the x values of the training data\n",
    "    Y_train -- A tensor of shape (2400,) that represents the y values of the training data\n",
    "    batch_num -- An integer representing the batch size to be used, in this case 1024\n",
    "    num_epoch -- An integer defining the number of epochs to be run, in this case 100\n",
    "    val_split -- A decimal representing the proportion of training data to be used as validation data\n",
    "    \n",
    "    Returns:\n",
    "    model -- The 3 layer Recurrent Neural Network that has been fitted to the training data\n",
    "    training_time -- An integer representing the amount of time (in seconds) that the model was training\n",
    "    \"\"\"\n",
    "    #Record the time the model starts training\n",
    "    start = time.time()\n",
    "\n",
    "    #Train the model on X_train and Y_train\n",
    "    model.fit(X_train, Y_train, batch_size= batch_num, nb_epoch=num_epoch, validation_split= val_split)\n",
    "\n",
    "    #Get the time it took to train the model (in seconds)\n",
    "    training_time = int(math.floor(time.time() - start))\n",
    "    return model, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Testing the Model\n",
    "\n",
    "- The models given x values of testing data & will predict normalized prices (y_predict)\n",
    "- Then, both the predicted values and the real values will be unnormalized and stored in separate arrays. \n",
    "- The values are unnormalized by looping through the predicted and true values. \n",
    "- 1 is added to each value, and then the result is multiplied by a corresponding number in the unnormalized_bases array. \n",
    "- In other words, the unnormalization processs is the exact reverse of the normalization process\n",
    "- Finally, a plot is created of the unnormalized real values and the unnormalized predicted values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(model, X_test, Y_test, unnormalized_bases):\n",
    "    \"\"\"\n",
    "    Test the model on the testing data\n",
    "    \n",
    "    Arguments:\n",
    "    model -- The previously fitted 3 layer Recurrent Neural Network\n",
    "    X_test -- A tensor of shape (267, 49, 35) that represents the x values of the testing data\n",
    "    Y_test -- A tensor of shape (267,) that represents the y values of the testing data\n",
    "    unnormalized_bases -- A tensor of shape (267,) that can be used to get unnormalized data points\n",
    "    \n",
    "    Returns:\n",
    "    y_predict -- A tensor of shape (267,) that represnts the normalized values that the model predicts based on X_test\n",
    "    real_y_test -- A tensor of shape (267,) that represents the actual prices of bitcoin throughout the testing period\n",
    "    real_y_predict -- A tensor of shape (267,) that represents the model's predicted prices of bitcoin\n",
    "    fig -- A branch of the graph of the real predicted prices of bitcoin versus the real prices of bitcoin\n",
    "    \"\"\"\n",
    "    #Test the model on X_Test\n",
    "    y_predict = model.predict(X_test)\n",
    "\n",
    "    #Create empty 2D arrays to store unnormalized values\n",
    "    real_y_test = np.zeros_like(Y_test)\n",
    "    real_y_predict = np.zeros_like(y_predict)\n",
    "\n",
    "    #Fill the 2D arrays with the real value and the predicted value by reversing the normalization process\n",
    "    for i in range(Y_test.shape[0]):\n",
    "        y = Y_test[i]\n",
    "        predict = y_predict[i]\n",
    "        real_y_test[i] = (y+1)*unnormalized_bases[i]\n",
    "        real_y_predict[i] = (predict+1)*unnormalized_bases[i]\n",
    "\n",
    "    #Plot of the predicted prices versus the real prices\n",
    "    fig = plt.figure(figsize=(10,5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(\"Bitcoin Price Over Time\")\n",
    "    plt.plot(real_y_predict, color = 'green', label = 'Predicted Price')\n",
    "    plt.plot(real_y_test, color = 'red', label = 'Real Price')\n",
    "    ax.set_ylabel(\"Price (USD)\")\n",
    "    ax.set_xlabel(\"Time (Days)\")\n",
    "    ax.legend()\n",
    "    \n",
    "    return y_predict, real_y_test, real_y_predict, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Evaluating Change in Price\n",
    "\n",
    "- Lets plot the model's predicted change in price each day against the real change in price daily\n",
    "- The percent increases of the predicted values and the real values are calculated by subtracting the value from the day before from the predicted/real value then dividing the result by 1+the value from the day before. \n",
    "- The predicted change in price is stored in delta_predict, while the real change in price is stored in delta_real.\n",
    "- These two tensors are then graphed together to visualize the difference between predicted and real change in price for bitcoin throughout the testing period. \n",
    "- The plot will represent the percent change in bitcoin price each day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def price_change(Y_daybefore, Y_test, y_predict):\n",
    "    \"\"\"\n",
    "    Calculate the percent change between each value and the day before\n",
    "    \n",
    "    Arguments:\n",
    "    Y_daybefore -- A tensor of shape (267,) that represents the prices of each day before each price in Y_test\n",
    "    Y_test -- A tensor of shape (267,) that represents the normalized y values of the testing data\n",
    "    y_predict -- A tensor of shape (267,) that represents the normalized y values of the model's predictions\n",
    "    \n",
    "    Returns:\n",
    "    Y_daybefore -- A tensor of shape (267, 1) that represents the prices of each day before each price in Y_test\n",
    "    Y_test -- A tensor of shape (267, 1) that represents the normalized y values of the testing data\n",
    "    delta_predict -- A tensor of shape (267, 1) that represents the difference between predicted and day before values\n",
    "    delta_real -- A tensor of shape (267, 1) that represents the difference between real and day before values\n",
    "    fig -- A plot representing percent change in bitcoin price per day,\n",
    "    \"\"\"\n",
    "    #Reshaping Y_daybefore and Y_test\n",
    "    Y_daybefore = np.reshape(Y_daybefore, (-1, 1))\n",
    "    Y_test = np.reshape(Y_test, (-1, 1))\n",
    "\n",
    "    #The difference between each predicted value and the value from the day before\n",
    "    delta_predict = (y_predict - Y_daybefore) / (1+Y_daybefore)\n",
    "\n",
    "    #The difference between each true value and the value from the day before\n",
    "    delta_real = (Y_test - Y_daybefore) / (1+Y_daybefore)\n",
    "\n",
    "    #Plotting the predicted percent change versus the real percent change\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title(\"Percent Change in Bitcoin Price Per Day\")\n",
    "    plt.plot(delta_predict, color='green', label = 'Predicted Percent Change')\n",
    "    plt.plot(delta_real, color='red', label = 'Real Percent Change')\n",
    "    plt.ylabel(\"Percent Change\")\n",
    "    plt.xlabel(\"Time (Days)\")\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return Y_daybefore, Y_test, delta_predict, delta_real, fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Process the Percent Change in Price\n",
    "\n",
    "- The percent change in price will be processsed such that an increase in price is represented by a 1, and a decrease/no change is represented by a 0. These binary values will be stored in arrays delta_predict_1_0 and delta_real_1_0. \n",
    "\n",
    "- This will be done by looping through the values of the real and predicted percent change arrays. If a value is greater than 0, a 1 is stored in a new array. Otherwise, a 0 is stored in the new array.\n",
    "\n",
    "- This process is very useful to understand how well the model did, and can be used to gather statistics about the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def binary_price(delta_predict, delta_real):\n",
    "    \"\"\"\n",
    "    Converts percent change to a binary 1 or 0, where 1 is an increase and 0 is a decrease/no change\n",
    "    \n",
    "    Arguments:\n",
    "    delta_predict -- A tensor of shape (267, 1) that represents the predicted percent change in price\n",
    "    delta_real -- A tensor of shape (267, 1) that represents the real percent change in price\n",
    "    \n",
    "    Returns:\n",
    "    delta_predict_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_predict\n",
    "    delta_real_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_real\n",
    "    \"\"\"\n",
    "    #Empty arrays where a 1 represents an increase in price and a 0 represents a decrease in price\n",
    "    delta_predict_1_0 = np.empty(delta_predict.shape)\n",
    "    delta_real_1_0 = np.empty(delta_real.shape)\n",
    "\n",
    "    #If the change in price is greater than zero, store it as a 1\n",
    "    #If the change in price is less than zero, store it as a 0\n",
    "    for i in range(delta_predict.shape[0]):\n",
    "        if delta_predict[i][0] > 0:\n",
    "            delta_predict_1_0[i][0] = 1\n",
    "        else:\n",
    "            delta_predict_1_0[i][0] = 0\n",
    "    for i in range(delta_real.shape[0]):\n",
    "        if delta_real[i][0] > 0:\n",
    "            delta_real_1_0[i][0] = 1\n",
    "        else:\n",
    "            delta_real_1_0[i][0] = 0    \n",
    "\n",
    "    return delta_predict_1_0, delta_real_1_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Comparing Predictions and Real Data\n",
    "\n",
    "The binary categories computed in the previous cell is now used to compare predicted and real data. It will be used to find the number of:\n",
    "- True positives\n",
    "- False positives\n",
    "- True negatives\n",
    "- False negatives\n",
    "These can then be used to further calculate statistics of the model's performance. \n",
    "\n",
    "This will be done by looping through both binary arrays at once and getting the corresponding values. If the real value is a 1 and the predicted value is a 1, that index will be counted as a true positive. If the real value is a 1 and the predicted value is a 0, that index will be counted as a false negative. If the real value is a 0 and the predicted value is a 0, that index will be counted as a true negative. If the real value is a 0 and the predicted value is a 1, that index will be counted as a false positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_positives_negatives(delta_predict_1_0, delta_real_1_0):\n",
    "    \"\"\"\n",
    "    Finding the number of false positives, false negatives, true positives, true negatives\n",
    "    \n",
    "    Arguments: \n",
    "    delta_predict_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_predict\n",
    "    delta_real_1_0 -- A tensor of shape (267, 1) that represents the binary version of delta_real\n",
    "    \n",
    "    Returns:\n",
    "    true_pos -- An integer that represents the number of true positives achieved by the model\n",
    "    false_pos -- An integer that represents the number of false positives achieved by the model\n",
    "    true_neg -- An integer that represents the number of true negatives achieved by the model\n",
    "    false_neg -- An integer that represents the number of false negatives achieved by the model\n",
    "    \"\"\"\n",
    "    #Finding the number of false positive/negatives and true positives/negatives\n",
    "    true_pos = 0\n",
    "    false_pos = 0\n",
    "    true_neg = 0\n",
    "    false_neg = 0\n",
    "    for i in range(delta_real_1_0.shape[0]):\n",
    "        real = delta_real_1_0[i][0]\n",
    "        predicted = delta_predict_1_0[i][0]\n",
    "        if real == 1:\n",
    "            if predicted == 1:\n",
    "                true_pos += 1\n",
    "            else:\n",
    "                false_neg += 1\n",
    "        elif real == 0:\n",
    "            if predicted == 0:\n",
    "                true_neg += 1\n",
    "            else:\n",
    "                false_pos += 1\n",
    "    return true_pos, false_pos, true_neg, false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Calculating Statistics\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Alexandros_Karatzoglou/publication/221515860/figure/fig1/AS:339586132791298@1457975051470/Figure-1-Mean-Squared-Error-formula-used-to-evaluate-the-user-model.ppm \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/qconrio-machinelearningforeveryone-150826200704-lva1-app6892/95/qcon-rio-machine-learning-for-everyone-51-638.jpg?cb=1440698161 \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "Putting everything together and getting statistics about the model. Statistics being calculated include:\n",
    "- Precision: How often the model gets a true positive compared to how often it returns a positive\n",
    "- Recall: How often the model gets a true positive compared to how often it should have gotten a positive\n",
    "- F1 Score: The weighted average of recall and precision\n",
    "- Mean Squared Error: The average of the squares of the differences between predicted and real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_statistics(true_pos, false_pos, true_neg, false_neg, y_predict, Y_test):\n",
    "    \"\"\"\n",
    "    Calculate various statistics to assess performance\n",
    "    \n",
    "    Arguments:\n",
    "    true_pos -- An integer that represents the number of true positives achieved by the model\n",
    "    false_pos -- An integer that represents the number of false positives achieved by the model\n",
    "    true_neg -- An integer that represents the number of true negatives achieved by the model\n",
    "    false_neg -- An integer that represents the number of false negatives achieved by the model\n",
    "    Y_test -- A tensor of shape (267, 1) that represents the normalized y values of the testing data\n",
    "    y_predict -- A tensor of shape (267, 1) that represents the normalized y values of the model's predictions\n",
    "    \n",
    "    Returns:\n",
    "    precision -- How often the model gets a true positive compared to how often it returns a positive\n",
    "    recall -- How often the model gets a true positive compared to how often is hould have gotten a positive\n",
    "    F1 -- The weighted average of recall and precision\n",
    "    Mean Squared Error -- The average of the squares of the differences between predicted and real values\n",
    "    \"\"\"\n",
    "    precision = float(true_pos) / (true_pos + false_pos)\n",
    "    recall = float(true_pos) / (true_pos + false_neg)\n",
    "    F1 = float(2 * precision * recall) / (precision + recall)\n",
    "    #Get Mean Squared Error\n",
    "    MSE = mean_squared_error(y_predict.flatten(), Y_test.flatten())\n",
    "\n",
    "    return precision, recall, F1, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 - Putting It All Together\n",
    "\n",
    "Applying all the methods defined above and analyzing results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120004, 49, 8)\n",
      "(120004,)\n",
      "(13334, 49, 8)\n",
      "(13334,)\n",
      "(13334,)\n",
      "(130938, 1)\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test, Y_daybefore, unnormalized_bases, window_size = load_data(r'C:\\Users\\angel\\Desktop\\Predict Model\\Bitcoin Data.csv', 50)\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_daybefore.shape)\n",
    "print(unnormalized_bases.shape)\n",
    "print(window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_4 (Bidirection (None, 49, 98)            22736     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 49, 98)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 49, 196)           154448    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 49, 196)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 98)                96432     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 99        \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 273,715\n",
      "Trainable params: 273,715\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model(window_size, 0.2, 'linear', 'mse', 'adam')\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\angel\\Anaconda2\\envs\\BIX\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 114003 samples, validate on 6001 samples\n",
      "Epoch 1/100\n",
      "114003/114003 [==============================] - 715s - loss: 0.0025 - val_loss: 6.3473e-05\n",
      "Epoch 2/100\n",
      "114003/114003 [==============================] - 668s - loss: 9.3285e-05 - val_loss: 2.5148e-05\n",
      "Epoch 3/100\n",
      "114003/114003 [==============================] - 647s - loss: 6.8649e-05 - val_loss: 2.0365e-05\n",
      "Epoch 4/100\n",
      "114003/114003 [==============================] - 647s - loss: 5.5362e-05 - val_loss: 1.7653e-05\n",
      "Epoch 5/100\n",
      "114003/114003 [==============================] - 649s - loss: 4.6875e-05 - val_loss: 1.6065e-05\n",
      "Epoch 6/100\n",
      "114003/114003 [==============================] - 650s - loss: 4.0196e-05 - val_loss: 1.5069e-05\n",
      "Epoch 7/100\n",
      "114003/114003 [==============================] - 648s - loss: 3.5167e-05 - val_loss: 1.3346e-05\n",
      "Epoch 8/100\n",
      "114003/114003 [==============================] - 645s - loss: 3.0638e-05 - val_loss: 1.2338e-05\n",
      "Epoch 9/100\n",
      "114003/114003 [==============================] - 648s - loss: 2.7397e-05 - val_loss: 1.1301e-05\n",
      "Epoch 10/100\n",
      "114003/114003 [==============================] - 737s - loss: 2.4910e-05 - val_loss: 1.0761e-05\n",
      "Epoch 11/100\n",
      "114003/114003 [==============================] - 649s - loss: 2.2301e-05 - val_loss: 9.7745e-06\n",
      "Epoch 12/100\n",
      "114003/114003 [==============================] - 646s - loss: 2.0376e-05 - val_loss: 8.8612e-06\n",
      "Epoch 13/100\n",
      "114003/114003 [==============================] - 644s - loss: 1.8243e-05 - val_loss: 8.5078e-06\n",
      "Epoch 14/100\n",
      "114003/114003 [==============================] - 644s - loss: 1.7049e-05 - val_loss: 8.0994e-06\n",
      "Epoch 15/100\n",
      "114003/114003 [==============================] - 644s - loss: 1.6000e-05 - val_loss: 7.4731e-06\n",
      "Epoch 16/100\n",
      "114003/114003 [==============================] - 645s - loss: 1.4732e-05 - val_loss: 7.3795e-06\n",
      "Epoch 17/100\n",
      "114003/114003 [==============================] - 646s - loss: 1.4256e-05 - val_loss: 7.1383e-06\n",
      "Epoch 18/100\n",
      "114003/114003 [==============================] - 673s - loss: 1.3835e-05 - val_loss: 7.2431e-06\n",
      "Epoch 19/100\n",
      "114003/114003 [==============================] - 649s - loss: 1.3479e-05 - val_loss: 6.9972e-06\n",
      "Epoch 20/100\n",
      "114003/114003 [==============================] - 646s - loss: 1.2337e-05 - val_loss: 6.4577e-06\n",
      "Epoch 21/100\n",
      "114003/114003 [==============================] - 649s - loss: 1.1810e-05 - val_loss: 6.2913e-06\n",
      "Epoch 22/100\n",
      "114003/114003 [==============================] - 650s - loss: 1.1376e-05 - val_loss: 6.5621e-06\n",
      "Epoch 23/100\n",
      "114003/114003 [==============================] - 650s - loss: 1.1031e-05 - val_loss: 6.0626e-06\n",
      "Epoch 24/100\n",
      "114003/114003 [==============================] - 648s - loss: 1.0587e-05 - val_loss: 6.1628e-06\n",
      "Epoch 25/100\n",
      "114003/114003 [==============================] - 650s - loss: 1.0151e-05 - val_loss: 5.9242e-06\n",
      "Epoch 26/100\n",
      "114003/114003 [==============================] - 649s - loss: 1.0138e-05 - val_loss: 7.9067e-06\n",
      "Epoch 27/100\n",
      "114003/114003 [==============================] - 647s - loss: 9.8132e-06 - val_loss: 5.4406e-06\n",
      "Epoch 28/100\n",
      "114003/114003 [==============================] - 649s - loss: 9.5070e-06 - val_loss: 5.7811e-06\n",
      "Epoch 29/100\n",
      "114003/114003 [==============================] - 649s - loss: 9.2759e-06 - val_loss: 5.1966e-06\n",
      "Epoch 30/100\n",
      "114003/114003 [==============================] - 646s - loss: 9.5132e-06 - val_loss: 5.5730e-06\n",
      "Epoch 31/100\n",
      "114003/114003 [==============================] - 649s - loss: 8.8789e-06 - val_loss: 5.0868e-06\n",
      "Epoch 32/100\n",
      "114003/114003 [==============================] - 650s - loss: 8.8251e-06 - val_loss: 4.9985e-06\n",
      "Epoch 33/100\n",
      "114003/114003 [==============================] - 648s - loss: 8.9594e-06 - val_loss: 8.4862e-06\n",
      "Epoch 34/100\n",
      "114003/114003 [==============================] - 647s - loss: 9.8991e-06 - val_loss: 6.4436e-06\n",
      "Epoch 35/100\n",
      "114003/114003 [==============================] - 657s - loss: 9.3743e-06 - val_loss: 5.1468e-06\n",
      "Epoch 36/100\n",
      "114003/114003 [==============================] - 651s - loss: 9.0867e-06 - val_loss: 7.1762e-06\n",
      "Epoch 37/100\n",
      "114003/114003 [==============================] - 650s - loss: 8.0336e-06 - val_loss: 4.4964e-06\n",
      "Epoch 38/100\n",
      "114003/114003 [==============================] - 651s - loss: 7.6021e-06 - val_loss: 4.8048e-06\n",
      "Epoch 39/100\n",
      "114003/114003 [==============================] - 650s - loss: 7.4242e-06 - val_loss: 5.7411e-06\n",
      "Epoch 40/100\n",
      "114003/114003 [==============================] - 655s - loss: 7.5396e-06 - val_loss: 5.2716e-06\n",
      "Epoch 41/100\n",
      "114003/114003 [==============================] - 648s - loss: 7.2878e-06 - val_loss: 4.4188e-06\n",
      "Epoch 42/100\n",
      "114003/114003 [==============================] - 652s - loss: 7.0905e-06 - val_loss: 4.2258e-06\n",
      "Epoch 43/100\n",
      "114003/114003 [==============================] - 652s - loss: 6.7843e-06 - val_loss: 4.2560e-06\n",
      "Epoch 44/100\n",
      "114003/114003 [==============================] - 649s - loss: 6.8047e-06 - val_loss: 4.2355e-06\n",
      "Epoch 45/100\n",
      "114003/114003 [==============================] - 653s - loss: 6.9789e-06 - val_loss: 5.1449e-06\n",
      "Epoch 46/100\n",
      "114003/114003 [==============================] - 650s - loss: 6.7117e-06 - val_loss: 3.9289e-06\n",
      "Epoch 47/100\n",
      "114003/114003 [==============================] - 653s - loss: 6.5450e-06 - val_loss: 4.5885e-06\n",
      "Epoch 48/100\n",
      "114003/114003 [==============================] - 654s - loss: 7.3441e-06 - val_loss: 4.1477e-06\n",
      "Epoch 49/100\n",
      "114003/114003 [==============================] - 651s - loss: 7.3076e-06 - val_loss: 4.2758e-06\n",
      "Epoch 50/100\n",
      "114003/114003 [==============================] - 650s - loss: 7.2356e-06 - val_loss: 4.6476e-06\n",
      "Epoch 51/100\n",
      "114003/114003 [==============================] - 654s - loss: 7.3191e-06 - val_loss: 4.3176e-06\n",
      "Epoch 52/100\n",
      "114003/114003 [==============================] - 645s - loss: 6.6261e-06 - val_loss: 5.6235e-06\n",
      "Epoch 53/100\n",
      "114003/114003 [==============================] - 652s - loss: 6.0237e-06 - val_loss: 4.4744e-06\n",
      "Epoch 54/100\n",
      "114003/114003 [==============================] - 650s - loss: 5.9689e-06 - val_loss: 4.0314e-06\n",
      "Epoch 55/100\n",
      "114003/114003 [==============================] - 669s - loss: 6.5008e-06 - val_loss: 4.9098e-06\n",
      "Epoch 56/100\n",
      "114003/114003 [==============================] - 650s - loss: 6.3230e-06 - val_loss: 3.8905e-06\n",
      "Epoch 57/100\n",
      "114003/114003 [==============================] - 647s - loss: 5.9745e-06 - val_loss: 3.8767e-06\n",
      "Epoch 58/100\n",
      "114003/114003 [==============================] - 645s - loss: 5.4880e-06 - val_loss: 3.4805e-06\n",
      "Epoch 59/100\n",
      "114003/114003 [==============================] - 645s - loss: 5.6688e-06 - val_loss: 5.3641e-06\n",
      "Epoch 60/100\n",
      "114003/114003 [==============================] - 648s - loss: 5.7095e-06 - val_loss: 5.0449e-06\n",
      "Epoch 61/100\n",
      "114003/114003 [==============================] - 650s - loss: 6.3005e-06 - val_loss: 4.8117e-06\n",
      "Epoch 62/100\n",
      "114003/114003 [==============================] - 667s - loss: 5.9643e-06 - val_loss: 4.2305e-06\n",
      "Epoch 63/100\n",
      "114003/114003 [==============================] - 653s - loss: 6.7625e-06 - val_loss: 5.4272e-06\n",
      "Epoch 64/100\n",
      "114003/114003 [==============================] - 655s - loss: 5.9773e-06 - val_loss: 4.1202e-06\n",
      "Epoch 65/100\n",
      "114003/114003 [==============================] - 653s - loss: 5.4408e-06 - val_loss: 3.2650e-06\n",
      "Epoch 66/100\n",
      "114003/114003 [==============================] - 655s - loss: 5.9336e-06 - val_loss: 3.5575e-06\n",
      "Epoch 67/100\n",
      "114003/114003 [==============================] - 649s - loss: 5.3245e-06 - val_loss: 3.0659e-06\n",
      "Epoch 68/100\n",
      "114003/114003 [==============================] - 652s - loss: 5.2569e-06 - val_loss: 9.2422e-06\n",
      "Epoch 69/100\n",
      "114003/114003 [==============================] - 647s - loss: 6.6088e-06 - val_loss: 3.3903e-06\n",
      "Epoch 70/100\n",
      "114003/114003 [==============================] - 652s - loss: 5.4302e-06 - val_loss: 4.2856e-06\n",
      "Epoch 71/100\n",
      "114003/114003 [==============================] - 650s - loss: 5.3120e-06 - val_loss: 4.2954e-06\n",
      "Epoch 72/100\n",
      "114003/114003 [==============================] - 650s - loss: 5.1599e-06 - val_loss: 3.2341e-06\n",
      "Epoch 73/100\n",
      "114003/114003 [==============================] - 655s - loss: 5.0303e-06 - val_loss: 3.0911e-06\n",
      "Epoch 74/100\n",
      "114003/114003 [==============================] - 666s - loss: 4.8417e-06 - val_loss: 3.8251e-06\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114003/114003 [==============================] - 688s - loss: 4.7528e-06 - val_loss: 2.9473e-06\n",
      "Epoch 76/100\n",
      "114003/114003 [==============================] - 657s - loss: 4.6956e-06 - val_loss: 3.0168e-06\n",
      "Epoch 77/100\n",
      "114003/114003 [==============================] - 643s - loss: 4.4298e-06 - val_loss: 2.9458e-06\n",
      "Epoch 78/100\n",
      "114003/114003 [==============================] - 643s - loss: 5.0722e-06 - val_loss: 3.1621e-06\n",
      "Epoch 79/100\n",
      "114003/114003 [==============================] - 649s - loss: 4.5013e-06 - val_loss: 2.9507e-06\n",
      "Epoch 80/100\n",
      "114003/114003 [==============================] - 647s - loss: 4.4445e-06 - val_loss: 2.9217e-06\n",
      "Epoch 81/100\n",
      "114003/114003 [==============================] - 648s - loss: 4.8012e-06 - val_loss: 2.9631e-06\n",
      "Epoch 82/100\n",
      "114003/114003 [==============================] - 648s - loss: 4.6444e-06 - val_loss: 2.7596e-06\n",
      "Epoch 83/100\n",
      "114003/114003 [==============================] - 654s - loss: 4.2277e-06 - val_loss: 3.0036e-06\n",
      "Epoch 84/100\n",
      "114003/114003 [==============================] - 673s - loss: 4.1505e-06 - val_loss: 2.9413e-06\n",
      "Epoch 85/100\n",
      "114003/114003 [==============================] - 651s - loss: 4.1360e-06 - val_loss: 3.0896e-06\n",
      "Epoch 86/100\n",
      "114003/114003 [==============================] - 652s - loss: 3.8321e-06 - val_loss: 2.7055e-06\n",
      "Epoch 87/100\n",
      "114003/114003 [==============================] - 653s - loss: 4.0717e-06 - val_loss: 2.9367e-06\n",
      "Epoch 88/100\n",
      "111616/114003 [============================>.] - ETA: 13s - loss: 3.8375e-06"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-93ce7091030c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#Print the training time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Training time\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_time\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"seconds\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-7085c6a34628>\u001b[0m in \u001b[0;36mfit_model\u001b[1;34m(model, X_train, Y_train, batch_num, num_epoch, val_split)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#Train the model on X_train and Y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mbatch_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mval_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m#Get the time it took to train the model (in seconds)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    958\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1657\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2357\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\BIX\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model, training_time = fit_model(model, X_train, Y_train, 1024, 100, .05)\n",
    "\n",
    "#Print the training time\n",
    "print (\"Training time\", training_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict, real_y_test, real_y_predict, fig1 = test_model(model, X_test, Y_test, unnormalized_bases)\n",
    "\n",
    "#Show the plot\n",
    "plt.show(fig1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Percent Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_daybefore, Y_test, delta_predict, delta_real, fig2 = price_change(Y_daybefore, Y_test, y_predict)\n",
    "\n",
    "#Show the plot\n",
    "plt.show(fig2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Binary Version of Percent Change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_predict_1_0, delta_real_1_0 = binary_price(delta_predict, delta_real)\n",
    "\n",
    "print delta_predict_1_0.shape\n",
    "print delta_real_1_0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Predictions and True Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos, false_pos, true_neg, false_neg = find_positives_negatives(delta_predict_1_0, delta_real_1_0)\n",
    "print \"True positives:\", true_pos\n",
    "print \"False positives:\", false_pos\n",
    "print \"True negatives:\", true_neg\n",
    "print \"False negatives:\", false_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision, recall, F1, MSE = calculate_statistics(true_pos, false_pos, true_neg, false_neg, y_predict, Y_test)\n",
    "print \"Precision:\", precision\n",
    "print \"Recall:\", recall\n",
    "print \"F1 score:\", F1\n",
    "print \"Mean Squared Error:\", MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BIX]",
   "language": "python",
   "name": "conda-env-BIX-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "360px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
